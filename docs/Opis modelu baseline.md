# 1 Co robi ten kod (baseline.py)?

To jest prosty **skrypt bazowy (baseline)** do sprawdzenia, *czy da siÄ™ przewidywaÄ‡ godzinÄ™ (`hour`) na podstawie Å›rednich kolorÃ³w RGB obrazu*. Innymi sÅ‚owy: â€czy sama informacja o tym, jak czerwone/zielone/niebieskie jest zdjÄ™cie, coÅ› mÃ³wi o porze dniaâ€.

PrzejdÅºmy przez to jak przez preparat pod mikroskopem ğŸ§ ğŸ”¬.

Najpierw importy.
Kod uÅ¼ywa NumPy i Pandas do pracy z danymi, scikit-learn do uczenia modelu (regresja logistyczna), oraz kilku metryk do oceny jakoÅ›ci klasyfikacji.

StaÅ‚a `FEATURES_PATH` wskazuje na plik `features_mean_rgb.csv`. To jest **cache z wczeÅ›niej policzonymi cechami** â€“ Å›rednie wartoÅ›ci R, G i B dla kaÅ¼dego obrazu.

Funkcja `main()` zaczyna od sprawdzenia, czy ten plik istnieje.
JeÅ›li nie â€“ program przerywa dziaÅ‚anie i mÃ³wi wprost: *najpierw uruchom skrypt, ktÃ³ry liczy Å›rednie RGB*. To zabezpieczenie przed trenowaniem modelu na â€niczymâ€.

Potem:

* CSV jest wczytywany do DataFrameâ€™a,
* wypisywany jest jego rozmiar (ile przykÅ‚adÃ³w i kolumn).

Z danych wyciÄ…gane sÄ…:

* `X`: macierz cech â†’ tylko trzy kolumny: `r_mean`, `g_mean`, `b_mean`,
* `y`: etykieta â†’ kolumna `hour` (najpewniej godzina wykonania zdjÄ™cia, np. 0â€“23).

Czyli model dostaje **3 liczby** i ma zgadnÄ…Ä‡ **jednÄ… z wielu klas (godzin)**. Bardzo minimalistyczne podejÅ›cie â€“ i o to chodzi w baseline.

NastÄ™pnie dane sÄ… dzielone:

* 70% na trening,
* 30% na test,
* `stratify=y` pilnuje, Å¼eby rozkÅ‚ad godzin byÅ‚ podobny w obu zbiorach,
* `random_state=42` gwarantuje powtarzalnoÅ›Ä‡ wynikÃ³w (magiczna liczba naukowcÃ³w).

Model:

* to **LogisticRegression**, ale w wersji wieloklasowej,
* `max_iter=5000` daje mu duÅ¼o czasu na zbieÅ¼noÅ›Ä‡,
* `n_jobs=-1` kaÅ¼e uÅ¼yÄ‡ wszystkich rdzeni CPU.

Model jest trenowany (`fit`), potem przewiduje godziny dla danych testowych (`predict`).

Na koniec kod:

* liczy accuracy (procent poprawnych predykcji),
* drukuje raport klasyfikacji (precision, recall, f1 dla kaÅ¼dej godziny),
* drukuje macierz pomyÅ‚ek (kto z kim siÄ™ myli).

PodsumowujÄ…c w jednym zdaniu:
**ten kod sprawdza, jak dobrze da siÄ™ przewidzieÄ‡ godzinÄ™ na podstawie samego â€koloruâ€ obrazÃ³w, uÅ¼ywajÄ…c bardzo prostego modelu jako punktu odniesienia**.

# 2 Czy to jest uczenie maszynowe (ML)?

To jest intelektualny odpowiednik pytania:
â€Zanim odpalimy rakietÄ™ neuronowÄ…, sprawdÅºmy, czy wystarczy rowerâ€.

JeÅ›li chcesz, kolejnym naturalnym krokiem jest porÃ³wnanie tego baselineâ€™u z:

* cechami bardziej zÅ‚oÅ¼onymi (histogramy, HSV),
* albo modelem nieliniowym (Random Forest, SVM),
* albo sieciÄ… neuronowÄ…, ktÃ³ra *widzi* coÅ› wiÄ™cej niÅ¼ Å›redniÄ… barwÄ™.

# 2 
Tak â€” **to jest model uczenia maszynowego**, i to w najczystszej, podrÄ™cznikowej postaci. ğŸ§ 

Konkretnie:

* uÅ¼ywa **uczenia nadzorowanego** (supervised learning),
* ma **dane wejÅ›ciowe** (`r_mean`, `g_mean`, `b_mean`),
* ma **etykiety** (`hour`),
* **uczy siÄ™ parametrÃ³w** na danych treningowych (`fit`),
* a potem **generalizuje** na danych testowych (`predict`).

Algorytm, ktÃ³ry tu pracuje, to **regresja logistyczna**. Mimo mylÄ…cej nazwy, w tej konfiguracji jest to **klasyfikator**, nie regresja w sensie â€przewidywania liczb ciÄ…gÅ‚ychâ€. Model uczy siÄ™ granic decyzyjnych w trÃ³jwymiarowej przestrzeni RGB i mÃ³wi: â€ten punkt najbardziej pasuje do godziny 14â€, â€ten do 7â€, itd.

WaÅ¼ny niuans filozoficzny (i praktyczny):
To jest **model bardzo prosty** i celowo â€gÅ‚upiâ€. Nie ma pojÄ™cia o ksztaÅ‚tach, obiektach ani sÅ‚oÅ„cu na horyzoncie. Widzi tylko Å›redni kolor caÅ‚ego obrazu. Ale wÅ‚aÅ›nie dlatego jest cenny â€” daje punkt odniesienia. JeÅ›li bardziej zÅ‚oÅ¼ony model nie przebije tego baselineâ€™u, to znaczy, Å¼e gdzieÅ› po drodze oszukujemy samych siebie.

Czyli:

* tak, to ML,
* tak, naprawdÄ™ siÄ™ uczy,
* nie, to nie jest â€AI z filmÃ³wâ€,
* i tak, jego prostota jest jego supermocÄ….

Dalej Å›wiat siÄ™ robi ciekawszy: moÅ¼na zapytaÄ‡, *dlaczego* w ogÃ³le kolor miaÅ‚by kodowaÄ‡ porÄ™ dnia i kiedy ta hipoteza zaczyna siÄ™ rozpadaÄ‡.

# 3 Jak dokÅ‚adnie dziaÅ‚a to â€uczenieâ€ w tym kodzie?
Åšwietne pytanie, bo tu dotykamy sedna â€uczeniaâ€ zamiast magii. Ten algorytm niczego **nie rozumie** o Å›wiecie. On tylko **dopasowuje liczby do liczb** w sprytny sposÃ³b. Zobaczmy, co naprawdÄ™ siÄ™ dzieje pod maskÄ….

To, co tu dziaÅ‚a, to **regresja logistyczna w wersji wieloklasowej**. Najpierw uproszczenie obrazu Å›wiata: kaÅ¼dy obraz zostaje sprowadzony do punktu w 3-wymiarowej przestrzeni. OÅ› X to Å›rednia czerwieni, Y â€“ zieleni, Z â€“ niebieskiego. KaÅ¼de zdjÄ™cie to jeden punkt w tej chmurze RGB.

Do kaÅ¼dego punktu przypisana jest etykieta: godzina. Algorytm dostaje wiÄ™c mnÃ³stwo przykÅ‚adÃ³w typu: â€ten punkt â†’ godzina 8â€, â€tamten punkt â†’ godzina 17â€.

Uczenie polega na znalezieniu **zestawu wag i biasÃ³w** (czyli liczb), ktÃ³re pozwalajÄ… jak najlepiej oddzieliÄ‡ punkty rÃ³Å¼nych godzin od siebie. Dla kaÅ¼dej godziny model buduje funkcjÄ™, ktÃ³ra mÃ³wi: â€jak bardzo ten punkt pasuje do mnieâ€. Matematycznie to jest prosta kombinacja liniowa: wagi razy RGB plus przesuniÄ™cie.

Te surowe wyniki sÄ… przepuszczane przez funkcjÄ™ softmax, ktÃ³ra zamienia je na coÅ› w rodzaju prawdopodobieÅ„stw. Model nie mÃ³wi tylko â€to jest 14â€, ale w tle liczy: â€14 ma 42%, 15 ma 30%, 13 ma 20%, reszta marginalnieâ€.

Kluczowy moment: **jak model siÄ™ poprawia?**
Na poczÄ…tku wagi sÄ… losowe, wiÄ™c odpowiedzi sÄ… kiepskie. Model porÃ³wnuje swoje przewidywania z prawdziwymi godzinami i liczy **funkcjÄ™ straty** (log-loss). Ta strata mÃ³wi jednÄ… liczbÄ…, jak bardzo model siÄ™ myli.

Potem uruchamia siÄ™ mechanizm uczenia wÅ‚aÅ›ciwego: **gradient descent**. Algorytm oblicza, w ktÃ³rÄ… stronÄ™ i o ile zmieniÄ‡ kaÅ¼dÄ… wagÄ™, Å¼eby strata byÅ‚a mniejsza. To jest czysta matematyka: pochodne, nachylenia, maÅ‚e kroki w dÃ³Å‚ â€zbocza bÅ‚Ä™duâ€.

Ten proces powtarza siÄ™ setki lub tysiÄ…ce razy:
â€“ policz przewidywania
â€“ policz bÅ‚Ä…d
â€“ popraw wagi
â€“ zrÃ³b to jeszcze raz

Uczenie koÅ„czy siÄ™, gdy model przestaje siÄ™ istotnie poprawiaÄ‡ albo dojdzie do limitu iteracji (`max_iter=5000`).

Efekt koÅ„cowy nie jest â€wiedzÄ… o porach dniaâ€. To jest **ukÅ‚ad pÅ‚aszczyzn decyzyjnych** przecinajÄ…cych przestrzeÅ„ RGB. KaÅ¼da pÅ‚aszczyzna to matematyczna granica: â€po tej stronie czÄ™Å›ciej byÅ‚a godzina 10, po tamtej 11â€.

Dlatego ten model:
â€“ jest szybki,
â€“ jest interpretowalny (moÅ¼na spojrzeÄ‡ na wagi),
â€“ ale jest Å›lepy na wszystko, co nieliniowe i przestrzenne.

Uczenie tutaj to nie odkrywanie praw natury, tylko **optymalizacja geometrii w trzech wymiarach**. A mimo to czasem dziaÅ‚a zaskakujÄ…co dobrze, bo Å›wiat bywa leniwy i koduje informacje tam, gdzie siÄ™ ich nie spodziewamy.
